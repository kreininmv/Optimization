{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZ5xtpQgyXIl",
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy\n",
    "\n",
    "from numpy.linalg import inv\n",
    "from jax import numpy as jnp\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prolem №1\n",
    "You will work with the following function for exercise, $f(x, y) = e^{-(sin(x) + cos(y))^2}$\n",
    "\n",
    "Draw the computational graph for the function. Note, that it should contain only primitive operations - you need to do it automatically."
   ],
   "metadata": {
    "id": "s0ObgRUUybJw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Function of first problem\n",
    "def func_p1(x, y):\n",
    "  return jnp.exp(- jnp.power((jnp.sin(x[0]) + jnp.cos(y[0])), 2))\n",
    "  \n",
    "\n",
    "def dfunc_p1(x, y):\n",
    "  return grad(func_p1, argnums=(0, 1))(x, y)"
   ],
   "metadata": {
    "id": "VJmtlvyxzThW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "z=jax.xla_computation(dfunc_p1)(numpy.random.rand(1), numpy.random.rand(1))\n",
    "\n",
    "with open(\"t1.txt\", \"w\") as f:\n",
    "  f.write(z.as_hlo_text())\n",
    "\n",
    "with open(\"t1.dot\", \"w\") as f:\n",
    "  f.write(z.as_hlo_dot_graph())"
   ],
   "metadata": {
    "id": "DdMgXEvCz_sA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "![picture]( https://drive.google.com/uc?id=1zsJdiVhAgIh3EiFwr84fham4JbfEAduI)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "dqV0NAK8CGgz",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem №2\n",
    "Compare analytic and autograd approach for the hessian of: $f(x) = \\frac{1}{2}x^TAx + b^Tx + c$"
   ],
   "metadata": {
    "id": "DY2KeCAQDWCA",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from jax import jacfwd, jacrev"
   ],
   "metadata": {
    "id": "ywEsX0dIEi9c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "A = numpy.random.rand(100, 100)\n",
    "b = numpy.random.rand(100)\n",
    "c = 1\n",
    "\n",
    "def func_p2(x):\n",
    " return 0.5 * x.T @ A @ x + b @ x + c  \n",
    "\n",
    "def hessian(f):\n",
    "  return jax.jacfwd(jax.grad(f))\n",
    "\n",
    "def d2func_p2(x):\n",
    "  return hessian(func_p2)(x)\n",
    "\n",
    "hessian_auto2 = d2func_p2(numpy.random.rand(100))\n",
    "hessian_anal2 = (A + A.T) / 2"
   ],
   "metadata": {
    "id": "3iUiry8WDGKo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Difference between autograde and analytical solution:"
   ],
   "metadata": {
    "id": "QcRxWM1pnhgg",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "numpy.linalg.norm(hessian_anal2 - hessian_auto2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGRaG6Nunmm1",
    "outputId": "865b170d-f3d6-4b6d-ca2b-921733ad7bee",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.1407686e-06"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cringe moment for visualising it"
   ],
   "metadata": {
    "id": "xQ2ngc9HnqK8",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z = jax.xla_computation(d2func_p2)(numpy.random.rand(100))\n",
    "\n",
    "with open(\"t2.txt\", \"w\") as f:\n",
    "   f.write(z.as_hlo_text())\n",
    "\n",
    "with open(\"t2.dot\", \"w\") as f:\n",
    "  f.write(z.as_hlo_dot_graph())"
   ],
   "metadata": {
    "id": "2lPeCzMmEknA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem №3\n",
    "Suppose we have the following function $f(x) = \\frac{1}{2} ||x||^2$, select a random point $x_0 \\in \\mathbb{B}^{1000} = \\{0 ≤ x_i ≤ 1 | ∀i \\}$. Consider 10 steps of the gradient descent starting from the point $x_0$: \n",
    "\n",
    "$x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$.\n",
    "\n",
    "Your goal in this problem to write the function, that takes 10 scalar values $\\alpha_i$ and return the result of the gradient descent on function $L = f(x_{10})$. And optimize this function using gradient descent on $\\alpha \\in \\mathbb{R}^{10}$. Suppose, $\\alpha_0 = 1$.\n",
    "\n",
    "$\\alpha_{k+1} = \\alpha_k - \\beta \\cdot \\frac{\\partial L}{\\partial \\alpha}$\n",
    "\n",
    "Choose any $\\beta$ and the number of steps your need. Describe obtained results."
   ],
   "metadata": {
    "id": "Ualj24G1IiiS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def func_p3(x):\n",
    "  return 0.5 * x.T @ x\n",
    "\n",
    "def dfunc_p3(x):\n",
    "  return grad(func_p3)(x)"
   ],
   "metadata": {
    "id": "jKPgt3dnJJZU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Do it later...\n",
    "def gradient(x0, alpha0, num_steps=10):\n",
    "  x = x0\n",
    "  alpha = alpha0\n",
    "  for i in range(0, num_steps):\n",
    "    x = x - alpha * dfunc_p3(x)"
   ],
   "metadata": {
    "id": "DT8txwvOLAwW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem №4\n",
    "Compare analytic and autograd approach for the gradient of: $f(X) = -log (det(X))$"
   ],
   "metadata": {
    "id": "cEGhVPkvdwTw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analytical gradient: $df = -\\frac{1}{det(X)} \\cdot det(X) \\langle X^{-T}, dX ⟩$\n",
    "\n",
    "$df = -⟨X^{-T}, dX⟩$\n",
    "\n",
    "$∇f = -X^{-T}$"
   ],
   "metadata": {
    "id": "ZvfXooouw3TP",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X = numpy.random.rand(100, 100)\n",
    "\n",
    "func_p4 = lambda X: -jnp.log(jnp.linalg.det(X))\n",
    "dfunc_p4 = lambda X: grad(func_p4)(X)\n",
    "\n",
    "grad_auto4 = dfunc_p4(X)\n",
    "grad_anal4 = -(inv(X)).T\n",
    "\n",
    "print(\"Difference between analytical and autograde methods:\", numpy.linalg.norm(grad_auto4 - grad_anal4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N224SdvVfuye",
    "outputId": "4175a550-a8c8-477c-ea94-4535866b2a1a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference between analytical and autograde methods: 0.00039553354\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem №5\n",
    "Compare analytic and autograd approach for the gradient and hessian of:\n",
    "$f(x) = x^Txx^Tx$"
   ],
   "metadata": {
    "id": "oGhEJLkyroYG",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def func_p5(x):\n",
    "  return jnp.dot(x.T, x)* jnp.dot(x.T, x)\n",
    "\n",
    "def dfunc_p5(x):\n",
    "  return grad(func_p5)(x)\n",
    "\n",
    "def d2func_p5(x):\n",
    "  return hessian(func_p5)(x)"
   ],
   "metadata": {
    "id": "w22DAJXqr5CQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analytical gradient:\n",
    "$df = 4 \\langle x, x \\rangle \\cdot \\langle x, dx ⟩ $\n",
    "\n",
    "$∇f = 4 ⟨x, x⟩ \\cdot x $\n",
    "\n",
    "Analytical hessian:\n",
    "$d^2f = 4 \\cdot \\big( ⟨dx_2, x⟩⋅⟨x, dx_1⟩ + ⟨x, dx_2⟩⋅⟨x, dx_1⟩ + ⟨x, x⟩⋅⟨dx_2, dx_1⟩\\big)$ \n",
    "\n",
    "$d^2f = 4 \\cdot x^T \\big(3 x \\cdot dx_2^T  \\big) dx_1 = 12 x^T \\cdot x dx_2^T \\cdot dx_1$\n",
    "\n",
    "$hessian(f) = 12 x \\cdot x^T$ - it will be matrix..."
   ],
   "metadata": {
    "id": "xyfwfoLTu07d",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = numpy.random.rand(2)\n",
    "#x = numpy.ones(2)\n",
    "grad_auto5 = grad(func_p5)(x)\n",
    "grad_anal5 = 4 * jnp.dot(x, x) * x\n",
    "\n",
    "print(\"Difference between analytic and auto gradient\",numpy.linalg.norm(grad_auto5 - grad_anal5))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_zLTGKdsAOT",
    "outputId": "c4f29d33-9821-43eb-a011-fbebb04ed9dc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference between analytic and auto gradient 0.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hessian_auto5 = d2func_p5(x)\n",
    "hessian_anal5 = 12 * jnp.outer(x, x.T)\n",
    "\n",
    "print(\"Difference between analytic and auto hessian:\",numpy.linalg.norm(hessian_auto5 - hessian_anal5))\n",
    "\n",
    "#print(\"Hessian auto: \", hessian_auto5, hessian_auto5.shape)\n",
    "#print(\"Hessian anal: \", hessian_anal5, hessian_anal5.shape)"
   ],
   "metadata": {
    "id": "kRhczohUwSe-",
    "outputId": "ed74fc6d-dc7d-41e0-de1e-122e71e8dc8f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Difference between analytic and auto hessian: 5.4479795\n"
     ]
    }
   ]
  }
 ]
}